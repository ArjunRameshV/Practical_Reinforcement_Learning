{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gettign started with Reinforcement Learning Environments using Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCZqa38nyIHOI6d88756my",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunRameshV/Practical_Reinforcement_Learning/blob/master/Gettign_started_with_Reinforcement_Learning_Environments_using_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS4WDQ2-caQe"
      },
      "source": [
        "## A brief Introduction\n",
        "\n",
        "Some of the main objectives:\n",
        "\n",
        "* Maximise the reward (maybe instantaneous or future ones). In general the sum of rewards over a certain time frame (also known as the return) \n",
        "* Environment, the problem representation \n",
        "* Agent, the algorithm representation. \n",
        "* Policy, a mapping between the states and possible actions. We do not know the best policy (i.e which action at a particular state gives the best reward) when starting out. The aim is to keep interacting with the environment to find an optimal policy. \n",
        "* The observation made by the agent may only be a subset of the environment's state (though they are used interchangeably at times) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD53uyeyeLjr"
      },
      "source": [
        "## The Cartpole Environment \n",
        "\n",
        "The \"Hello World\" to reinforcement learning. \n",
        "\n",
        "Here, we have a pole fixed to a cart, that moves on a frictionless surface. The pole is initially inclined at 90<sup>o</sup> to the surface of the cart. The goal is to prevent the pole from falling over by controlling the cart. \n",
        "\n",
        "In the classical setup, the state of the environment, made available to the cart is a 4D vector consisting of the position, velocity of the cart and the angle of inclination and angular velocity of the pole.\n",
        "\n",
        "Possible actions that can be taken by the agent are: +1 (move right) or -1 (move left)\n",
        "\n",
        "A reward 1 is provided for every timestep the pole remains upright. We come to an end (an episode ends) when :\n",
        "\n",
        "    * The pole tips over some angle limit \n",
        "    * The cart moved outside the world's boundaries \n",
        "    * The time step is 200\n",
        "\n",
        "The goal is to learn a policy $\\pi(a_{s}|s_{t})$, that finds the maximum return (sum of rewards) in an episode $\\sum_{t=0}^{T}\\gamma^{t}r_{t}$. Here $\\gamma$ is called the discount factor and has a value between [0,1]. The main reason for using gamma is to make sure that we prioritize immediate rewards over future ones so that the best policy is found quickly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsiPnX48fNIa"
      },
      "source": [
        "**Making an environment** \n",
        "\n",
        "In general, environments can be made in python or tensorflow. One possible things we will look into is to make an environment in python and use a wrapper to convert it into tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpXzbsDTjnLf"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P4tBTTqjsky",
        "outputId": "65204e0a-4d12-45ec-a64c-dc5ef75f11dc"
      },
      "source": [
        "!pip install -q tf-agents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 10.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 16.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 17.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 10.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 9.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 737kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 808kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 880kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 952kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 9.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnActTGgjvZm"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z06B3NekmhI"
      },
      "source": [
        "import abc \n",
        "import tensorflow as tf \n",
        "import numpy as np "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtsTpQURkstp"
      },
      "source": [
        "from tf_agents.environments import py_environment \n",
        "from tf_agents.environments import tf_environment \n",
        "from tf_agents.environments import tf_py_environment \n",
        "from tf_agents.environments import utils \n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers \n",
        "from tf_agents.environments import suite_gym \n",
        "from tf_agents.trajectories import time_step as ts "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESzdSEMglHwf"
      },
      "source": [
        "# to make sure we use tensorflow 2 functions \n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gRVapKElS8H"
      },
      "source": [
        "### Python Environment \n",
        "\n",
        "The environment should have a function, that takes an action from the agent for the current state and returns the next state along with a reward. In general we need a **step(action) --> next_time_step** that returns:\n",
        " \n",
        "* **observations**: The part of the environment that can be seen by the agent. \n",
        "* **reward**: A return from the environment after an agent takes an action. \n",
        "* **step_type**: Interactions with an environment are usually part of an episode. This is used to indicate whether this time step is the first (`FIRST`), intermediary (`MID`) or last (`LAST`) in the episode. \n",
        "* **discount**: A float representing how much to weight the reward at the next timestep to the current reward. \n",
        "\n",
        "These are grouped into a tuple `TimeStep(step_type, reward, discount, observation).`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FXFgSBIlqc2"
      },
      "source": [
        "\n",
        "```\n",
        "class PyEnvironment(object):\n",
        "  def __init__(self):\n",
        "    # return the initial time step \n",
        "    self._current_time_step = self._reset()\n",
        "    return self._current_time_step \n",
        "\n",
        "  def step(self, action):\n",
        "    # apply the action and return the next step \n",
        "    if self._current_time_step is None:\n",
        "      return self.reset()\n",
        "    self._current_time_step = self._step(action)\n",
        "    return self._current_time_step\n",
        "  \n",
        "  def current_time_step(self):\n",
        "    return self._current_time_step\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    # returns the time_step_spec\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self):\n",
        "    # return observation_spec\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self):\n",
        "    # return action_spec\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    # resets the inital_time_step\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    # apply action and return the new time_step\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y3Dxp6b50lX"
      },
      "source": [
        "### Understanding a standard environment \n",
        "\n",
        "Tensorlfow agents have built-in wrappers for many standard environments like Open-AI Gym. Lets see how these wrapped environments look like and the structure of the action and time_step_spec they follow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBGHrk3z6d4H"
      },
      "source": [
        "environment = suite_gym.load(\"CartPole-v0\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9Bsc07E6dpa",
        "outputId": "577fbaff-5dd7-4a06-a314-04d0cc92629e"
      },
      "source": [
        "print('action_spec: ', environment.action_spec())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action_spec:  BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LAVPkDW6zSr",
        "outputId": "3644f14b-67be-44db-d0f5-588b464e5ada"
      },
      "source": [
        "# the observation available to the agent at each time_step\n",
        "print('time_step_spec.observation: ', environment.time_step_spec().observation)\n",
        "# the type of step at this time_step (the inital, intermediary or final)\n",
        "print('time_step_spec.step_type: ', environment.time_step_spec().step_type)\n",
        "# the gamma factor in return \n",
        "print('time_step_spec.discount: ', environment.time_step_spec().discount)\n",
        "# the reward if action is made from a particular state \n",
        "print('time_step_spec.reward: ', environment.time_step_spec().reward)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time_step_spec.observation:  BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
            "time_step_spec.step_type:  ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
            "time_step_spec.discount:  BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
            "time_step_spec.reward:  ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBKd6_ly6zK1",
        "outputId": "8dc959a5-928b-429b-a228-8f3c76bd6b8a"
      },
      "source": [
        "# observaing what happens when we take a fixed action \n",
        "action = np.array(1, dtype=np.int32)\n",
        "time_step = environment.reset()\n",
        "print(\"The initial time_step: \", time_step)\n",
        "while not time_step.is_last():\n",
        "  # the agent keeps making the same action\n",
        "  time_step = environment.step(action)\n",
        "  print(time_step)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The initial time_step:  TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01817294,  0.00269327,  0.0018532 , -0.00895948], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.0182268 ,  0.1977886 ,  0.00167401, -0.30105713], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.02218258,  0.39288664, -0.00434713, -0.59321165], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03004031,  0.5880692 , -0.01621137, -0.88726074], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.04180169,  0.7834074 , -0.03395658, -1.1849954 ], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.05746984,  0.97895294, -0.05765649, -1.488126  ], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.0770489 ,  1.1747279 , -0.08741901, -1.7982421 ], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.10054345,  1.3707124 , -0.12338385, -2.1167648 ], dtype=float32))\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.1279577 ,  1.5668306 , -0.16571915, -2.4448886 ], dtype=float32))\n",
            "TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([ 0.15929432,  1.7629335 , -0.21461692, -2.7835116 ], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClUbTRI1Xdn"
      },
      "source": [
        "##A custom Black-Jack Environment\n",
        "\n",
        "Let's construct a simple environment to play the game of black-jack. \\\\\n",
        "In this game, we have an infinite deck of cards numbered from 1 to 10. \\\\\n",
        "At every turn, the agent can either take a new random card or stop the current round.  \\\\\n",
        " The goal of the game is to get a sum as close to 21 as possible at the end of the round while not exceeding 21.\n",
        "\n",
        " \\\\\n",
        "\n",
        " A possible way to construct the environment is by using the following representations:\n",
        "\n",
        " * **action**: 0 for taking a new card and 1 to terminate the current round. \n",
        "\n",
        " * **observation**: (the current state of the environment, as seen by the agent) sum of cards in the current round (since the deck is considered as an infinite one, we dont have any advantage if tracking the cards. If the number of cards where finite, then each individual cards could have been included in the observation) \n",
        "\n",
        " * **reward**: At the end of each episode,\n",
        "> The objective is to get as close to 21 as possible. \n",
        ">      If sum_of_cards <= 21: \n",
        "          then sum_of_cards-21\n",
        "        else:\n",
        "          then -21 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyrIK4RU5YMN"
      },
      "source": [
        "class BlackJack(py_environment.PyEnvironment):\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
        "    self._state = 0\n",
        "    self._episode_ended = False \n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    # if the last action ended the episode\n",
        "    if self._episode_ended: \n",
        "      return self.reset()\n",
        "\n",
        "    # make sure the episodes terminate \n",
        "    if action == 1:\n",
        "      self._episode_ended = True\n",
        "    elif action == 0:\n",
        "      new_card = np.random.randint(1, 11)\n",
        "      self._state += new_card \n",
        "    else:\n",
        "      raise ValueError(' action should be either 0 or 1')\n",
        "\n",
        "    if self._episode_ended or self._state == 21:\n",
        "      reward = self._state - 21 if self._state <= 21 else -21 \n",
        "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
        "    else: \n",
        "      return ts.transition(np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuPwEpJW5i2_"
      },
      "source": [
        "Some things to keep in mind when creating an environment, the observations and the time_steps generated follow the correct shapes and types as defined the the specs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI_LCyMmEjnz"
      },
      "source": [
        "# creating a mock run with random policy that iterates over 5 episodes \n",
        "environment = BlackJack()\n",
        "utils.validate_py_environment(environment=environment, episodes=5)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wJwPGh7HkJd"
      },
      "source": [
        "**Test Run**: Use a fixed policy to generate 3 cards and then end the turn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU5953z6JDZw"
      },
      "source": [
        "# since action=0 generates a new card and action=1 terminates the current episode\n",
        "get_new_card_action = np.array(0, dtype=np.int32)\n",
        "end_round_action = np.array(1, dtype=np.int32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch9kBrO2JP66",
        "outputId": "ef557018-4dfa-4f96-c7f3-4f76c90fdce7"
      },
      "source": [
        "environment = BlackJack()\n",
        "# an initial reset\n",
        "time_step = environment.reset()\n",
        "print(\"The initial time step: \" , time_step)\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for i in range(3):\n",
        "  time_step = environment.step(get_new_card_action)\n",
        "  print(f'The {i+1}th time_step: {time_step}')\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = environment.step(end_round_action)\n",
        "print(\"The terminal time_step: \", time_step)\n",
        "cumulative_reward += time_step.reward \n",
        "print(f\"The final reward: {cumulative_reward}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The initial time step:  TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0], dtype=int32))\n",
            "The 1th time_step: TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([7], dtype=int32))\n",
            "The 2th time_step: TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([16], dtype=int32))\n",
            "The 3th time_step: TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([17], dtype=int32))\n",
            "The terminal time_step:  TimeStep(step_type=array(2, dtype=int32), reward=array(-4., dtype=float32), discount=array(0., dtype=float32), observation=array([17], dtype=int32))\n",
            "The final reward: -4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9PHlNoxKhRQ"
      },
      "source": [
        "##Environment Wrapper \n",
        "\n",
        "An environment wrapper takes a python environment and returns a modified environment, that is also an instance of py_environment.PyEnvironment. \n",
        "\n",
        "Some common wrappers: \n",
        "* ActionDiscretizeWrapper: Converts a continuous action space into a discrete action space. \n",
        "* RunsStats: Captures the run time statistics of an environment such as number of steps taken, number of episodes completed. \n",
        "* TimeLimit: Terminates the episode after a fixed amount of steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dr5QNR9RnUj"
      },
      "source": [
        "\n",
        "An example of Action Discretize wrapper, used to discretize the continuous action space for invertedPendulum (a PyBullet environment, that accepts action from the continuous range [-2,2]). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPxKrisVSMQx",
        "outputId": "7264d0e8-8d5b-45e0-93f5-d680ca1b4612"
      },
      "source": [
        "env = suite_gym.load(\"Pendulum-v0\")\n",
        "print(\"Action specification: \", env.action_spec())\n",
        "\n",
        "discrete_action_env = wrappers.ActionDiscretizeWrapper(env,num_actions=5)\n",
        "print(\"Discretized action specification: \", discrete_action_env.action_spec())\n",
        "\n",
        "print(f\"\\nThe initial data-type: {env.action_spec().dtype} and the data-type after wrapping: {discrete_action_env.action_spec().dtype}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action specification:  BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-2.0, maximum=2.0)\n",
            "Discretized action specification:  BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=4)\n",
            "\n",
            "The initial data-type: float32 and the data-type after wrapping: int32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIuAjZMATbcj"
      },
      "source": [
        "## TensorFlow Environments\n",
        "\n",
        "It is very much similar to the python environment, except for the facts that:\n",
        "\n",
        "* Tensor objects are generated instead of arrays \n",
        "* TF environments adds batch dimension to the tensor generated when compared to the python env specs. (An additional dimension will be present when compared to the above examples) \n",
        "\n",
        "Similar to the python environment, we have abstract methods that can be overridden or used an abstract methods. The current_time_step() can also initialize the environment if needed. \n",
        "\n",
        "```\n",
        "class TFEnvironment(object):\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    # descirbes the time_step tensor returned by step()\n",
        "\n",
        "  def observation_spec(self):\n",
        "    # defines the tensor spec for observation provided by the environment \n",
        "\n",
        "  def action_spec(self):\n",
        "    # describes the tensor spec of the action expected by the step(action) \n",
        "\n",
        "  def reset(self):\n",
        "    # returns the current time_step after reseting the environment \n",
        "    return self._reset()\n",
        "\n",
        "  def current_time_step(self):\n",
        "    # returns the current time_step\n",
        "    return self._current_time_step()\n",
        "\n",
        "  def step(self, action):\n",
        "    # applies the action and returns the new time_step \n",
        "    return self._step(action) \n",
        "\n",
        "  @abc.abstractMethod\n",
        "  def _reset(self):\n",
        "    # returns the current time_step after reseting the environment\n",
        "\n",
        "  @abc,abstractMethod\n",
        "  def _current_time_step(self):\n",
        "    # returns the current time_step \n",
        "\n",
        "  @abc.abstractMethod\n",
        "  def _step(self, action):\n",
        "    # applies the action and returns the new time_step\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hrsaKxCTo9d"
      },
      "source": [
        "## Wrapping a python environmetn in tensorflow \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifxec5VHYDeK",
        "outputId": "77c3f0a1-2a2f-4267-a5c1-8f53cfcbd9af"
      },
      "source": [
        "env = BlackJack()\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
        "print(\"Python env, time_step spec: \", env.time_step_spec())\n",
        "print(\"TF env, time_step_spec: \", tf_env.time_step_spec())\n",
        "print(\"\\n\")\n",
        "print(\"Python env, action spec: \", env.action_spec())\n",
        "print(\"TF env, action_spec: \", tf_env.action_spec())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Python env, time_step spec:  TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(1,), dtype=dtype('int32'), name='observation', minimum=0, maximum=2147483647))\n",
            "TF env, time_step_spec:  TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(2147483647, dtype=int32)))\n",
            "\n",
            "\n",
            "Python env, action spec:  BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=1)\n",
            "TF env, action_spec:  BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvteI6_AZMs4"
      },
      "source": [
        " A few things to observe from above:\n",
        "\n",
        " * The change from BoundedArraySpec to BoundedTensorSpec\n",
        " * The datatype changes from int and float to tf.int and tf.float\n",
        " * We can see the additional dimension in minimum and maximum values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ_1pwH_Y7x1",
        "outputId": "6fd61fdf-46ab-41f2-bb92-cf3b0c70571f"
      },
      "source": [
        "# doing a quick run, using the same policy as before\n",
        "\n",
        "get_new_card_action = tf.constant([0], dtype=tf.int32)\n",
        "end_round_action = tf.constant([1], dtype=tf.int32)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "num_of_cards_drawn_in_one_episode = 3\n",
        "cumulative_reward = 0\n",
        "\n",
        "for i in range(num_of_cards_drawn_in_one_episode):\n",
        "  time_step = env.step(get_new_card_action)\n",
        "  print(f\"{i+1}th time_step: {time_step}\")\n",
        "  cumulative_reward += time_step.reward \n",
        "\n",
        "time_step = tf_env.step(end_round_action)\n",
        "print(f\"Final time_step: {time_step}\")\n",
        "cumulative_reward += time_step.reward \n",
        "print(\"The final reward: \", cumulative_reward)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1th time_step: TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([10], dtype=int32))\n",
            "2th time_step: TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([16], dtype=int32))\n",
            "3th time_step: TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([19], dtype=int32))\n",
            "Final time_step: TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[19]], dtype=int32)>)\n",
            "The final reward:  tf.Tensor([-2.], shape=(1,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVTchmtheG3m"
      },
      "source": [
        "## Running a wrapped environment of the cart-pole\n",
        "\n",
        "We try to run an example of the cart-pole (explained after the inroduction section) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE_rPSNYh_cn"
      },
      "source": [
        "# loading the environment \n",
        "env = suite_gym.load(\"CartPole-v0\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXH2iZllpSIc"
      },
      "source": [
        "# wrapping it to a tensor environment \n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62buOe9CjE_r",
        "outputId": "6050d031-50e8-4fc5-c52f-c1611a3bd650"
      },
      "source": [
        "# simulating one episode \n",
        "\n",
        "time_step = tf_env.reset() \n",
        "num_steps = 3 \n",
        "transitions = []\n",
        "reward = 0\n",
        "\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i%2])   # decide an action, move right or left \n",
        "  next_time_step = tf_env.step(action)   # take the action \n",
        "  transitions.append([time_step, action, next_time_step])   # remember the transition information \n",
        "  reward += next_time_step.reward   # update the rewad \n",
        "  time_step = next_time_step    # update the time_step variable\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print(\"\\n\".join(map(str, np_transitions)))\n",
        "print(f\"Total reward: {reward.numpy()}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TimeStep(step_type=array([0], dtype=int32), reward=array([0.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02394745, -0.04264867,  0.0013553 , -0.00630255]],\n",
            "      dtype=float32)), array([0], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02309448, -0.23779003,  0.00122925,  0.2868077 ]],\n",
            "      dtype=float32))]\n",
            "[TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.02309448, -0.23779003,  0.00122925,  0.2868077 ]],\n",
            "      dtype=float32)), array([1], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.01833868, -0.04268564,  0.00696541, -0.0054873 ]],\n",
            "      dtype=float32))]\n",
            "[TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.01833868, -0.04268564,  0.00696541, -0.0054873 ]],\n",
            "      dtype=float32)), array([0], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.01748497, -0.23790678,  0.00685566,  0.28938514]],\n",
            "      dtype=float32))]\n",
            "Total reward: [3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogXb2gT9ov0c",
        "outputId": "52a8847e-b1db-42c2-b433-ff628d59f708"
      },
      "source": [
        "# running multiple episodes \n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for i in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform(shape=[1], dtype=tf.int32, minval=0, maxval=2)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy() \n",
        "  print(f\"Episode {i+1}, reward: {episode_reward} and #steps: {episode_steps}\")\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print(f\"total num of episodes: {num_episodes}, total number of steps: {num_steps}\")\n",
        "print(f\"average length of steps: {avg_length}, average reward: {avg_reward}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 1, reward: [10.] and #steps: 10\n",
            "Episode 2, reward: [38.] and #steps: 38\n",
            "Episode 3, reward: [17.] and #steps: 17\n",
            "Episode 4, reward: [19.] and #steps: 19\n",
            "Episode 5, reward: [15.] and #steps: 15\n",
            "total num of episodes: 5, total number of steps: 99\n",
            "average length of steps: 19.8, average reward: 19.799999237060547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmh_3VmZrIcT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}